---
title: "Exploratory Data Analysis"
author: "Mauro Lepore"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 6
vignette: >
  %\VignetteIndexEntry{Exploratory Data Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
# hadley's settings
set.seed(1014)
options(digits = 3)

knitr::opts_chunk$set(
  echo = TRUE,  # {mine}
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = 'center',
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

options(dplyr.print_min = 6, dplyr.print_max = 6)
```



# Introduction

This vignette will show you how to explore your data systematically, using modern and more powerful tools for visualization and transformation. An exploratory data analysis is an iterative process that aims to use data and some research questions to learn something that can help refine the questions and move forward the learning spiral.



## Prerequisites

The data we will use comes from Barro Colorado Island. It is available at DOI: https://doi.org/10.5479/data.bci.20130603 and via the __[bci](https://forestgeo.github.io/bci/)__ data package.

Some functions we will use are available in the __[forestr](https://forestgeo.github.io/forestr/)__ package, and many are available in the __tidyverse__ package (which includes, for example, the packages __[dplyr](http://dplyr.tidyverse.org/)__ and __[ggplot2](http://ggplot2.tidyverse.org/)__). If you are new to the __tidyversre__, you may feel a little frustrated at the begining. But quickly your effort will pay-off. The tools in the __tidyverse__ are powerful, realatively easy to use and consistent; so once you learn some, you will learn most other tools intuitively.

To more compleately learan how to do data science with the __tidyverse__ read [R for Data Science](http://r4ds.had.co.nz/), by Garrett Grolemund and Hadley Wickham.

```{r packages}
# To install packages see ?install.packages and ?devtools::install_github
# Data
library(bci)  # to access data from BCI
# Functions
library(forestr)  # to analyse forest dynamics
library(tidyverse)  # to visualize and transform data, and much more
library(forcats)  # tools for dealing with categorical variables
```



## Style

### Being explicit about what package a function comes from

Frequently I refer to functions from a particular package as `package::fun()` (e.g. the first time I use a function). Althought the prefix `package::` is unnecesary if the package has previously been loaded with `library(package)`, I will often keep that prefix to be explicit about where the function comes from. This should help you to find the function if you ever want to use it again.



### The tidyverse style guide

I use [The tidyverse style guide](http://style.tidyverse.org/).

> Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.

--Hadley Wickham, [The tidyverse style guide](http://style.tidyverse.org/index.html).



### Combining multiple operations with the pipe `%>%`

Often I combine multiple operations with the pipe `%>%` because it makes the code considerably more readable.

> [The pipe] focuses on the transformations, not what's being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce `%>%` when reading code is "then".

> Behind the scenes, `x %>% f(y)` turns into `f(x, y)`, and `x %>% f(y) %>% g(z)` turns into `g(f(x, y), z)` and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom.

> Working with the pipe is one of the key criteria for belonging to the tidyverse. The only exception is ggplot2: it was written before the pipe was discovered.

--[Combining multiple operations with the pipe](http://r4ds.had.co.nz/transform.html)



## Definitions

> A __variable__ is a quantity, quality, or property that you can measure.

> A __value__ is the state of a variable when you measure it. The value of a
    variable may change from measurement to measurement.
  
> An __observation__ is a set of measurements made under similar conditions
    (you usually make all of the measurements in an observation at the same 
    time and on the same object). An observation will contain several values, 
    each associated with a different variable. I'll sometimes refer to 
    an observation as a data point.

> __Tabular data__ is a set of values, each associated with a variable and an 
    observation. Tabular data is _tidy_ if each value is placed in its own
    "cell", each variable in its own column, and each observation in its own 
    row.

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

# Questions

To start exploring your data, you can generally ask:

> What type of variation occurs within my variables?

> What type of covariation occurs between my variables? [^2



# Variation

> __Variation__ is the tendency of the values of a variable to change from measurement to measurement. 

Every variable varies with a particular pattern, and that pattern may be insightful. To understand the variation pattern of a variable we can visualize the distribution of the variables' values. The best way to visualize a variable's distribution depends on whether the variable is categorical or continuous.

The BCI data has both, categorical and continuous variables. In R, categorical variables are usually stored as character strings (<chr>) or factors (fctr), and continuous variables are stored as integers (<int>) or doubles (<dbl>).

These are the first few rows of the data FullViewTable census 7 from Barro Colorado Island, released in 2012:

```{r viz-data}
head(bci::bci12full7)
```

If you want to handle the data faster and print it nicer, convert dataframes to tibbles:

```{r}
bci12full7 <- tibble::as_tibble(bci12full7)  
bci12full7
```


NOTES: 

1. If you ever want to print more rows than tibble's default use:
   print(your_tibble, n = rows_n)
2. If when you use tibbles with forestr functions you get this error message:
   "Can't use matrix or array for column indexing", please read "Interacting
   with legacy code" at https://goo.gl/CJ5jLs.
3. For alternative views try:

```R
as.data.frame(bci12full7)  # takes long and doesn't print nicely
View(bci12full7)  # prints to a viewer panel
str(bci12full7)  # informative and prints nice
dplyr::glimpse(bci12full7)  # like str() but shows as much data as possible
```



## Visualizing Distributions

### Categorical variables

> A variable is categorical if it can only take one of a small set of values.

> To examine the distribution of a categorical variable, use a bar chart.

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).



Let's use a bar chart to explore the variable `status`, which represents the status of the tree or stem (see description of `ViewFullTable` variables at the [data dictionary](https://goo.gl/wGKEE3)). Possible values are: alive (tree or stem is alive), dead (tree is dead), lost_stem (stem is dead, not found, or broken, etc. but other stems of the tree are still alive), or missing (tree or stem was not found, so measurement is unknown).

```{r viz-categorical-bar, fig.cap="Values distribution of the categorical variable `status`"}
ggplot2::ggplot(data = bci12full7) +
  ggplot2::geom_bar(mapping = aes(x = status))
```

Although the [data dictionary](https://goo.gl/wGKEE3) does not explicitely refer to `A`, `D` and `M`, you likely know or guess that they mean alive, dead and misisng. To confirm this, let's first count the the number of unique values of the variables `status` and `DFstatus` (`DFstatus` is not decribed in the [data dictionary](https://goo.gl/wGKEE3)), then joining both counts by the shared variable `n`:

```{r introduce-count}
count_status <- dplyr::count(bci12full7, status)
count_DFstatus <- dplyr::count(bci12full7, DFstatus)
dplyr::full_join(count_status, count_DFstatus)
```



### Continuous Variables

> A variable is continuous if it can take any of an infinite set of ordered values.

> To examine the distribution of a continuous variable use a histogram.

> You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns.

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).



Let's use a histogram to explore the variable `dbh`, which represents the stem diameter, usually at breast height (see description of `ViewFullTable` variables at the [data dictionary](https://goo.gl/wGKEE3)).

In this section we will focus on small trees; big trees will be explored later. And we will try bars of different widths and choose one for further analyses.

```{r viz-continuous-hist, fig.show="hold"}
small_dbh <- dplyr::filter(bci12full7, dbh < 500)
gg_small_dbh <- ggplot(data = small_dbh, mapping = aes(x = dbh))
gg_small_dbh + geom_histogram(binwidth = 10)
```

```{r}
gg_small_dbh + geom_histogram(aes(dbh), binwidth = 30)
```

```{r}
gg_small_dbh + geom_histogram(aes(dbh), binwidth = 60)
```

A bar width of 30 `dbh` seems useful.

```{r set-barwidth}
useful_barwidth <- 30
```

To overlay multiple histograms in the same plot, `geom_freqpoly()` may produce clearer plots than `geom_histogram()` because it is easier to understand overlying lines than bars.

```{r hist-vs-freqpoly-compare, fig.cap="For multiple histograms, lines are easier to understand than bars."}
# Make n groups with ~ numbers of observations with `ggplot2::cut_number()`
small_cut_num <- mutate(small_dbh, equal_n = cut_number(dbh, n = 5))

# Now, compare the next two plots:
ggplot(small_cut_num) + geom_histogram(aes(dbh, color = equal_n))
ggplot(small_cut_num) + geom_freqpoly(aes(dbh, color = equal_n))
```

### Typical and rare Values

In both bar charts and histograms, tall and short bars let us explore common and less-common values. For example, we could ask:

> - Which values are the most common? Why?

> - Which values are rare? Why? Does that match your expectations?

> - Can you see any unusual patterns? What might explain them?

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

The later two plots give a good estimate of what tree diameters are most common. You can compute the same count manually, by cutting the variable with `ggplot2::cut_width()` and then counting the unique pieces with `dplyr::count()`:

```{r}
small_cut_width <- dplyr::mutate(
  small_dbh,
  dbh_cut = ggplot2::cut_width(dbh, width = useful_barwidth)
)
small_cut_width %>% dplyr::count(dbh_cut, sort = TRUE)
```

Although the most common trees are those which diameter (`dbh`) is between 15 mm and 45 mm, smaller trees are generally more common than bigger trees, which we already learned from previous plots.



### Clustered values

In this section we will focus on the largest trees; later we will explore all trees, including medium-size ones.

```{r}
largest_dbh <- filter(bci12full7, dbh > 2000)
```

> Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask:

> - How are the observations within each cluster similar to each other?

> - How are the observations in separate clusters different from each other?

> - How can you explain or describe the clusters?

> - Why might the appearance of clusters be misleading?

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

In the next plot, however, the clustering may be an artifact of the chosen bar-width; the clustering dissapears with bars twice as wide.

```{r clustering, fig.show="hold", fig.align="default"}
gg_largest_dbh <- ggplot(largest_dbh, aes(dbh))
gg_largest_dbh + geom_histogram(binwidth = useful_barwidth)
gg_largest_dbh + geom_histogram(binwidth = useful_barwidth * 2)
```



### Unusual values

In this section we will work with the entire bci12full7 dataset.

> Outliers are observations that are unusual; data points that don't seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram. 

> For example, take the distribution of the _dbh_ variable of the _bci12full7_ dataset. The only evidence of outliers is the unusually wide limits on the x-axis (stress mine).

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

```{r}
bci_hist <- ggplot(bci12full7, aes(x = dbh)) +
  geom_histogram(binwidth = useful_barwidth)
bci_hist
```   

> There are so many observations in the common bins that the rare bins are so short that you can't see them . To make it easy to see the unusual values, we need to zoom to small values of the y-axis with `coord_cartesian()`:

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

```{r}
bci_hist + coord_cartesian(ylim = c(0, 50))
```   

> (`coord_cartesian()` also has an `xlim()` argument for when you need to zoom into the x-axis. ggplot2 also has `xlim()` and `ylim()` functions that work slightly differently: they throw away the data outside the limits.)

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

This allows us to see that `dbh` values over 2000 are unusual. We pluck them out with `dplyr::filter()` and select a subset of informative variables:

```{r}
unusual_trees <- bci12full7 %>% 
  filter(dbh > 2000) %>% 
  dplyr::select(treeID, ExactDate, status, gx, gy, dbh) %>%
  dplyr::arrange(dbh)
unusual_trees
```

You could plot the unusual trees to see where they are located. A number of the unusually large trees seem to be on the edge of the plot, where `gy` values are lower than 100 (left plot below). This information helps us to filter those edge trees selectively (right plot below).

```{r locate-unusual-trees, fig.align="default", fig.show="hold"}
# For faster analysis and to avoid overplotting, choose only 100 trees at random
sample_100 <- bci12full7 %>% dplyr::sample_n(100)

# Left plot
unusual_tree_loc <- ggplot(sample_100, aes(gx, gy)) + 
    # plot a few points for reference with only 1/10 opacity (less distracting)
    geom_point(alpha = 1/10) +
    # highlight location of unusual trees in the plot
    geom_point(data = unusual_trees, colour = "red")
unusual_tree_loc

# Right plot
edge_trees <- unusual_trees %>% filter(gy < 100)
unusual_tree_loc + 
  geom_point(
    data = edge_trees, 
    size = 3, shape = 1  # highlight edge trees with bigger and hollow points
  )
```

Now you can decide what to do with the unusual and edge trees.

> It's good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can't figure out why they're there, it's reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn't drop them without justification. You'll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.

-- [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).

# Missing Values

This section comes from [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).), except references to data, plots and minor edits to text.


If you've encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options.

1.  Drop the entire row with the strange values:

    ```{r}
are_usual <- !bci12full7$treeID %in% unusual_trees$treeID
usual_trees <- bci12full7 %>% filter(are_usual)

# Confirm data set of usual trees has less rows than full data set.
bci12full7 %>% nrow()
usual_trees %>% nrow()
```
    
    I don't recommend this option because just because one measurement
    is invalid, doesn't mean all the measurements are. Additionally, if you
    have low quality data, by time that you've applied this approach to every
    variable you might find that you don't have any data left!

1.  Instead, I recommend replacing the unusual values with missing values.
    The easiest way to do this is to use `mutate()` to replace the variable
    with a modified copy. You can use the `ifelse()` function to replace
    unusual values with `NA`:

    ```{r}
are_unusual <- !are_usual
with_unusual_made_NA <- bci12full7 %>% 
  mutate(dbh = ifelse(are_unusual, NA_real_, dbh))
```

`ifelse()` has three arguments. The first argument `test` should be a logical vector. The result will contain the value of the second argument, `yes`, when `test` is `TRUE`, and the value of the third argument, `no`, when it is false.

Note (mine): Alternatively to ifelse, use `dplyr::case_when()`.

> `case_when()` is particularly useful inside mutate when you want to
  create a new variable that relies on a complex combination of existing
  variables

--`dplyr::case_when()`

```{r}
# Confirm no rows have been removed,
bci12full7 %>% nrow()
with_unusual_made_NA %>% nrow()

# but dbh of unusual trees is NA
unusual_only <- with_unusual_made_NA %>% 
  filter(are_unusual) %>% 
  select(dbh, treeID)
unusual_only
```

Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. It's not obvious where you should plot missing values, so ggplot2 doesn't include them in the plot, but it does warn that they've been removed (left plot below). To suppress that warning, set `na.rm = TRUE` (right plot below).

```{r, fig.align="default", fig.show="hold"}
# Left plot
ggplot(unusual_only, aes(dbh)) + 
  geom_histogram(binwidth = useful_barwidth) +
  labs(title = "Expect empty plot but get a warning")

# Right plot
ggplot(unusual_only, aes(dbh)) + 
  geom_histogram(binwidth = useful_barwidth, na.rm = TRUE) +
  labs(title = "Expect empty plot but no warning")
```

Other times you want to understand what makes observations with missing values different to observations with recorded values.

For example, in `bci::bci12full7`, some missing values in the `dbh` variable correspond to alive trees. So you might want to compare the `DFstatus` for trees with missing and non-misisng values of `dbh`. You can do this by making a new variable with `is.na()`.

```{r}
missing_dbh_trees <- bci12full7 %>% mutate(missing_dbh = is.na(dbh))
missing_dbh_trees %>% 
  ggplot() + 
    geom_bar(aes(DFstatus, fill = missing_dbh))
```

To know why `dbh` is missing in alive trees, you may want to further explore all variables with a single unique value in this conditions (i.e. `dbh = NA` and `DFstatus = "alive"`):

```{r}
has_single_unique_value <- function(x) {dplyr::n_distinct(x) == 1}
missing_dbh_trees %>% 
  filter(missing_dbh == TRUE, DFstatus == "alive") %>% 
  select_if(has_single_unique_value) %>% 
  unique()
```

## Exercises

1.  What happens to missing values in a histogram?  What happens to missing
    values in a bar chart? Why is there a difference?

1.  What does `na.rm = TRUE` do in `mean()` and `sum()`?











# Covariation

This section comes from [R for Data Science, Garrett Grolemund and Hadley Wickham](http://r4ds.had.co.nz/).), except references to data, plots and minor edits to text.

If variation describes the behavior _within_ a variable, covariation describes the behavior _between_ variables. **Covariation** is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualise the relationship between two or more variables. How you do that should again depend on the type of variables involved.

## A categorical and continuous variable

It's common to want to explore the distribution of a continuous variable broken down by a categorical variable. The default appearance of `geom_freqpoly()` is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it's hard to see the differences in shape. 
For example, let's explore how stem diameter (`dbh`) varies with species (`sp`):

```{r}
# Show the problem with `count` using a few species as example

a_few_species <- bci12full7 %>% 
  select(sp) %>% 
  unique() %>% 
  pull() %>% 
  .[1:5]
data_few_sp <- bci12full7 %>% filter(sp %in% a_few_species)

data_few_sp %>% 
  ggplot(aes(dbh)) +
    geom_freqpoly(aes(color = sp))
```

It's hard to see the difference in distribution because the overall counts differ so much:

```{r}
ggplot(data_few_sp) + 
  geom_bar(mapping = aes(
    # reorder `sp` from highest to lowest frequency
    x = forcats::fct_infreq(sp)
  )
)
```

To make the comparison easier we need to swap what is displayed on the y-axis. Instead of displaying count, we'll display __density__, which is the count standardised so that the area under each frequency polygon is one.

```{r}
data_few_sp %>% 
  ggplot(aes(x= dbh, y = ..density..)) +
    geom_freqpoly(aes(color = sp))
```

Another alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. A **boxplot** is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of:

* A box that stretches from the 25th percentile of the distribution to the 
  75th percentile, a distance known as the interquartile range (IQR). In the
  middle of the box is a line that displays the median, i.e. 50th percentile,
  of the distribution. These three lines give you a sense of the spread of the
  distribution and whether or not the distribution is symmetric about the
  median or skewed to one side. 

* Visual points that display observations that fall more than 1.5 times the 
  IQR from either edge of the box. These outlying points are unusual
  so are plotted individually.

* A line (or whisker) that extends from each end of the box and goes to the   
  farthest non-outlier point in the distribution.

```{r, echo = FALSE, out.width = "100%"}
knitr::include_graphics("https://raw.githubusercontent.com/hadley/r4ds/master/images/EDA-boxplot.png")
```

Let's take a look at the distribution of tree diameter (`dbh`) by species (`sp`) using `geom_boxplot()`:

```{r fig.height = 3}
ggplot(data = data_few_sp, mapping = aes(x = sp, y = dbh)) +
  geom_boxplot()
```

We see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot).

Above we ordered the variable `sp` by its frequency. But now, to make the trend easier to see, we can reorder `sp` based on the median value of `dbh`. One way to do that is with the `reorder()` function.

```{r}
data_few_sp <- data_few_sp %>% 
  mutate(sp = reorder(sp, dbh, FUN = median, na.rm = TRUE))

data_few_sp %>% 
  ggplot(mapping = aes(x = sp, y = dbh)) + 
  geom_boxplot()
```

If you have long variable names, `geom_boxplot()` will work better if you flip it 90°. You can do that with `coord_flip()` (left plot below), or with `ggstance::geom_boxploth()` and swaping `x` and `y` mappings (right plot below):

```{r}
# Left plot
ggplot2::last_plot() +  # reusing previous plot
  ggplot2::coord_flip()

# Right plot
data_few_sp %>% 
  ggplot() +
  ggstance::geom_boxploth(aes(x = dbh, y = sp))  # swap x, y; compare to above
```

### Alternatives

One problem with boxplots is that they were developed in an era of 
much smaller datasets and tend to display a prohibitively large
number of "outlying values". One approach to remedy this problem is
the letter value plot.

```{r}
# install.packages("lvplot")
ggplot(data_few_sp, aes(sp, dbh)) + 
  lvplot::geom_lv()
```

Compare and contrast `geom_violin()` with a facetted `geom_histogram()`,
or a coloured `geom_freqpoly()`. What are the pros and cons of each 
method?

```{r, out.width="50%"}
ggplot(data_few_sp, aes(sp, dbh)) + 
  geom_violin()

ggplot(data_few_sp, aes(dbh)) + 
  geom_histogram() +
  facet_wrap(~sp)
```

If you have a small dataset, it's sometimes useful to use `geom_jitter()`
to see the relationship between a continuous and categorical variable.
The ggbeeswarm package provides a number of methods similar to 
`geom_jitter()`.

```{r, out.width="33%"}
small_dataset <- data_few_sp %>% group_by(sp) %>% sample_n(50)

p <- ggplot(small_dataset, aes(sp, dbh))
p + geom_point()
p + geom_jitter()
p + ggbeeswarm::geom_quasirandom()
```

## Two categorical variables

To visualise the covariation between categorical variables, you'll need to count the number of observations for each combination. One way to do that is to rely on the built-in `geom_count()`. The size of each circle in the plot displays how many observations occurred at each combination of values (left plot below). Covariation will appear as a strong correlation between specific x values and specific y values.

```{r, out.width="50%"}
# Left: simple count
ggplot(data = data_few_sp) +
  geom_count(mapping = aes(x = sp, y = DFstatus))
```

To more clearly show the distribution of `DFstatus` within `sp` (or `sp` within `DFstatus`) you can map bubble size to a proportion calculated over `sp` (or over `DFstatus`):

```{r}
# Right: proportion; columns sum to 1.
ggplot(data_few_sp, aes(x = sp, y = DFstatus)) +
  geom_count(aes(size = ..prop.., group = sp)) +
  scale_size_area(max_size = 10)
```

Another approach is to compute the count with dplyr:

```{r}
few_spp_n <- data_few_sp %>% count(sp, DFstatus)
few_spp_n
```

Then visualise with `geom_tile()` and the fill aesthetic:

```{r}
few_spp_n %>% 
  ggplot(aes(sp, DFstatus)) +
    geom_tile(mapping = aes(fill = n))
```

## Two continuous variables

You've already seen one great way to visualise the covariation between two continuous variables: draw a scatterplot with `geom_point()`. You can see covariation as a pattern in the points. For example, you can see an exponential relationship between the tree diameter (`dbh`) and the above ground biomass (`agb`).

```{r, echo=FALSE}
set.seed(1234)
```

```{r, dev = "png"}
large_dataset <- bci12full7 %>% 
  filter(dbh < 400) %>% 
  sample_n(100000)
ggplot(data = large_dataset) +
  geom_point(mapping = aes(x = dbh, y = agb))
```

Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above).
You've already seen one way to fix the problem: using the `alpha` aesthetic to add transparency.

```{r, dev = "png"}
ggplot(data = large_dataset) +
  geom_point(mapping = aes(x = dbh, y = agb), alpha = 1 / 50)
```

But using transparency can be challenging for very large datasets. Another solution is to use bin. Previously you used `geom_histogram()` and `geom_freqpoly()` to bin in one dimension. Now you'll learn how to use `geom_bin2d()` and `geom_hex()` to bin in two dimensions.

`geom_bin2d()` and `geom_hex()` divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. `geom_bin2d()` creates rectangular bins. `geom_hex()` creates hexagonal bins. You will need to install the hexbin package to use `geom_hex()`.

```{r, fig.asp = 1, out.width = "50%", message = FALSE}
ggplot(data = large_dataset) +
  geom_bin2d(mapping = aes(x = dbh, y = agb))

# install.packages("hexbin")
ggplot(data = large_dataset) +
  geom_hex(mapping = aes(x = dbh, y = agb))
```

Another option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. For example, you could bin `dbh` and then for each group, display a boxplot:

```{r}
ggplot(data = large_dataset, mapping = aes(x = dbh, y = agb)) + 
  geom_boxplot(mapping = aes(group = cut_width(dbh, useful_barwidth)))
```

`cut_width(x, width)`, as used above, divides `x` into bins of width `width`. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it's difficult to tell that each boxplot summarises a different number of points. One way to show that is to make the width of the boxplot proportional to the number of points with `varwidth = TRUE`.

Another approach is to display approximately the same number of points in each bin. That's the job of `cut_number()`:

```{r}
ggplot(data = large_dataset, mapping = aes(x = dbh, y = agb)) + 
  geom_boxplot(mapping = aes(group = cut_number(dbh, 20)))
```

## Patterns and models

xxxcont.


Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:

+ Could this pattern be due to coincidence (i.e. random chance)?

+ How can you describe the relationship implied by the pattern?

+ How strong is the relationship implied by the pattern?

+ What other variables might affect the relationship?

+ Does the relationship change if you look at individual subgroups of the data?

A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above.

```{r fig.height = 2}
ggplot(data = faithful) + 
  geom_point(mapping = aes(x = eruptions, y = waiting))
``` 

Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.

Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It's hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It's possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts `price` from `carat` and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. 

```{r, dev = "png"}
library(modelr)

mod <- lm(log(price) ~ log(carat), data = diamonds)

diamonds2 <- diamonds %>% 
  add_residuals(mod) %>% 
  mutate(resid = exp(resid))

ggplot(data = diamonds2) + 
  geom_point(mapping = aes(x = carat, y = resid))
```

Once you've removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive. 

```{r}
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

You'll learn how models, and the modelr package, work in the final part of the book, [model](#model-intro). We're saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand.

## ggplot2 calls

As we move on from these introductory chapters, we'll transition to a more concise expression of ggplot2 code. So far we've been very explicit, which is helpful when you are learning:

```{r, eval = FALSE}
ggplot(data = faithful, mapping = aes(x = eruptions)) + 
  geom_freqpoly(binwidth = 0.25)
```

Typically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to `ggplot()` are `data` and `mapping`, and the first two arguments to `aes()` are `x` and `y`. In the remainder of the book, we won't supply those names. That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what's different between plots. That's a really important programming concern that we'll come back in [functions].

Rewriting the previous plot more concisely yields:

```{r, eval = FALSE}
ggplot(faithful, aes(eruptions)) + 
  geom_freqpoly(binwidth = 0.25)
```

Sometimes we'll turn the end of a pipeline of data transformation into a plot. Watch for the transition from `%>%` to `+`. I wish this transition wasn't necessary but unfortunately ggplot2 was created before the pipe was discovered.

```{r, eval = FALSE}
diamonds %>% 
  count(cut, clarity) %>% 
  ggplot(aes(clarity, cut, fill = n)) + 
    geom_tile()
```

## Learning more

If you want to learn more about the mechanics of ggplot2, I'd highly recommend grabbing a copy of the ggplot2 book: <https://amzn.com/331924275X>. It's been recently updated, so it includes dplyr and tidyr code, and has much more space to explore all the facets of visualisation. Unfortunately the book isn't generally available for free, but if you have a connection to a university you can probably get an electronic version for free through SpringerLink.

Another useful resource is the [_R Graphics Cookbook_](https://amzn.com/1449316956) by Winston Chang. Much of the contents are available online at <http://www.cookbook-r.com/Graphs/>.

I also recommend [_Graphical Data Analysis with R_](https://amzn.com/1498715230), by Antony Unwin. This is a book-length treatment similar to the material covered in this chapter, but has the space to go into much greater depth. 













# To consider

## Working with Dates

The variable `date` is the number of days since 1960-01-01 (see data dictionary at https://goo.gl/Q6XYrb), but it will be easier to interpret if we convert it to a date-time object (see `?lubridate::as_datetime`).

```{r}
library(lubridate)

# "Duration" is a useful intermediate; learn more with ?lubridate::duration
# %/%: integere diviison removes useless and annoying fraction of seconds.
bci12full7 <- mutate(bci12full7, duration = dseconds((date * 24 * 60 * 60) %/% 1))
bci12full7 <- mutate(bci12full7, datetime = as_datetime(duration, origin = "1960-01-01"))
bci12full7 %>% 
  select(date, datetime)
```

```{r}
bci12full7$date <- lubridate::as_date(bci12full7$date, origin = "1960-01-01")
bci12full7 %>% 
  select(date) %>% 
  glimpse()
```

## Tools to explore data quickly

### inzight light

http://lite.docker.stat.auckland.ac.nz/

### skimr

```{r}
library(skimr)  # xxx remove or declare in DESCRIPTION in Suggest:
skim(bci12full7) %>% filter(stat == "hist") %>% as.matrix()
```

To do all of the above quickly, we can use package __skimr__ (https://github.com/ropenscilabs/skimr).

```{r, eval=FALSE}
library(skimr)
smry <- skim(diamonds)
smry %>% 
  dplyr::filter(stat == "hist") %>% 
  as.matrix()  # Only needed in Windows for histograms (https://goo.gl/S8MaZW)
```
